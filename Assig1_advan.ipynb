{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmarking Algorithms and Feature Extractors using fetch20newsgroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import Libraries and Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/millenasiqueiraguimaraes/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/millenasiqueiraguimaraes/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/millenasiqueiraguimaraes/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1427 documents\n",
      "2 categories\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from gensim.models import Word2Vec, Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import os\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Step 1: Load dataset\n",
    "categories = ['alt.atheism', 'talk.religion.misc']\n",
    "newsgroups = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
    "print(f\"{len(newsgroups.filenames)} documents\")\n",
    "print(f\"{len(newsgroups.target_names)} categories\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Pre-process the text: Vectorize train and test data, clean is a custom defined function for pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = [' '.join(clean(text)) for text in newsgroups.data]\n",
    "y = newsgroups.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to replace negative values with zero "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_negatives(X):\n",
    "    return np.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Extract features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec_transform(X):\n",
    "    word2vec_model = Word2Vec(sentences=X, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in X])\n",
    "    return X_word2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_data = [TaggedDocument(words=words, tags=[str(i)]) for i, words in enumerate(X_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec_transform(X):\n",
    "    doc2vec_model = Doc2Vec(tagged_data, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    X_doc2vec = np.array([doc2vec_model.infer_vector(doc_words=words.split()) for words in X])\n",
    "    return X_doc2vec\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. train the classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines = {\n",
    "    'Word2Vec + LogisticRegression': Pipeline([\n",
    "        ('w2v', FunctionTransformer(word2vec_transform, validate=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=1000))\n",
    "    ]),\n",
    "    'Word2Vec + SVC': Pipeline([\n",
    "        ('w2v', FunctionTransformer(word2vec_transform, validate=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', SVC())\n",
    "    ]),\n",
    "    'Word2Vec + DecisionTree': Pipeline([\n",
    "        ('w2v', FunctionTransformer(word2vec_transform, validate=False)),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ]),\n",
    "    'Doc2Vec + LogisticRegression': Pipeline([\n",
    "        ('d2v', FunctionTransformer(doc2vec_transform, validate=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', LogisticRegression(max_iter=1000))\n",
    "    ]),\n",
    "    'Doc2Vec + SVC': Pipeline([\n",
    "        ('d2v', FunctionTransformer(doc2vec_transform, validate=False)),\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('clf', SVC())\n",
    "    ]),\n",
    "    'Doc2Vec + DecisionTree': Pipeline([\n",
    "        ('d2v', FunctionTransformer(doc2vec_transform, validate=False)),\n",
    "        ('clf', DecisionTreeClassifier())\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the classifier: grid search and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store classification reports\n",
    "classification_reports = {}\n",
    "\n",
    "# Grid search and evaluation\n",
    "for pipeline_name, pipeline in pipelines.items():\n",
    "    # Fit pipeline\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    # Generate classification report\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Store classification report in dictionary\n",
    "    classification_reports[pipeline_name] = report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save report as txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification reports saved to /Users/millenasiqueiraguimaraes/Library/Mobile Documents/com~apple~CloudDocs/Courses/GBC material/Machine Learning II/classification_reports.txt\n"
     ]
    }
   ],
   "source": [
    "home_dir = os.path.expanduser('/Users/millenasiqueiraguimaraes/Library/Mobile Documents/com~apple~CloudDocs/Courses/GBC material/Machine Learning II')\n",
    "output_file = os.path.join(home_dir, \"classification_reports.txt\")\n",
    "\n",
    "with open(output_file, \"w\") as file:\n",
    "    for pipeline_name, report in classification_reports.items():\n",
    "        file.write(f\"Pipeline: {pipeline_name}\\n\")\n",
    "        file.write(classification_report(y_test, y_pred))\n",
    "        file.write(\"\\n\\n\")\n",
    "\n",
    "print(f\"Classification reports saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
